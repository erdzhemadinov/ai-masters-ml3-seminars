{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6b4467-61fe-4cb7-b862-c615d15b95da",
   "metadata": {
    "id": "cd6b4467-61fe-4cb7-b862-c615d15b95da"
   },
   "source": [
    "## План\n",
    "\n",
    "0. План\n",
    "1. Организационные вопросы\n",
    "2. Bird-eye view\n",
    "3. Производные и градиенты\n",
    "4. Векторы и матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c96e5c-0d28-46c7-b15f-d16abf52cf15",
   "metadata": {
    "id": "69c96e5c-0d28-46c7-b15f-d16abf52cf15"
   },
   "source": [
    "## 1. Организационные вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tNu_PVjq0FHk",
   "metadata": {
    "id": "tNu_PVjq0FHk"
   },
   "source": [
    "* 1 лекция -> 1 семинар\n",
    "  * По плану 15 штук\n",
    "* Для сдачи курса нужны баллы \n",
    "  * Как получить?\n",
    "    * 5 домашек x 20 баллов (+ бонусные баллы)\n",
    "      * 3 тетрадки\n",
    "      * 2 соревнования\n",
    "      * **Дедлайны жесткие**\n",
    "    * Возможно (!) короткие квизы перед семинарами\n",
    "  * Сколько надо?\n",
    "    * Оценка вычисляется как `grade = (18 * points / 100 - 6).round().clip(0, 10)` (см. ниже)\n",
    "    * Зачет - с тройки\n",
    "    * Минимум 4 домашки должны быть сданы не меньше, чем на 10 баллов\n",
    "* За жульничество - санкции (штрафы или того хуже). Жульничать - это, например:\n",
    "  * Копировать чужой код в тетрадках (и давать другим)\n",
    "    * Если брали из опенсорса - указывайте это\n",
    "  * Использовать чужие сабмиты в соревнованиях\n",
    "  * Решать за других квизы\n",
    "  * И все такое\n",
    "* Связь в телеграме\n",
    "* Все получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dDI3tGyA1mvA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "dDI3tGyA1mvA",
    "outputId": "5481ff7c-92f7-4038-de59-dbbf0b5dfa14"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.arange(0, 100)\n",
    "\n",
    "def grade(x):\n",
    "    return (18 * x / 100 - 6).round().clip(0, 10)\n",
    "\n",
    "ys = grade(xs)\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"points\")\n",
    "plt.ylabel(\"grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41028e1e-b939-422c-8184-6bcc109be440",
   "metadata": {
    "id": "41028e1e-b939-422c-8184-6bcc109be440"
   },
   "source": [
    "## 2. Bird-eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fa4d5-ae74-45a6-bacd-60a0d8c9e189",
   "metadata": {
    "id": "c81fa4d5-ae74-45a6-bacd-60a0d8c9e189"
   },
   "source": [
    "Модели ML - это параметризованные функции, которые (как правило) в качестве аргумента принимают объекты данных: \n",
    "* Векторы признаков, \n",
    "* Тензоры изображений / аудио / ..., \n",
    "* Последовательности токенов для текстов, ...\n",
    "\n",
    "Выходы функции интерпретируются как \"предсказания\" (*predictions*), которые вместе с известными правильными ответами (*ground truth*) используются для корректировки параметров функции в процессе обучения.\n",
    "\n",
    "![BEV](https://i.ibb.co/jyG18Kx/aim-seminar01-completed-1-2.png)\n",
    "\n",
    "Для того, чтобы численно оценить, насколько ответы модели соответствуют правде, используются функции потерь (*loss functions*):\n",
    "* Для регрессии: \n",
    "  * Mean Squared Error: $MSE(y, \\hat{y}) = ||y - \\hat{y}||_2^2$,\n",
    "  * Mean Absolute Error: $MAE(y, \\hat{y}) = |y - \\hat{y}|$,\n",
    "  * ...\n",
    "* Для классификации:\n",
    "  * (Binary) Cross Entropy Loss: $BCE(y, \\hat{y}) = -y \\times \\log(\\hat{y}) - (1 - y) \\times \\log(1 - \\hat{y})$\n",
    "  * Hinge Loss, Focal Loss, \\<YOUR LOSS\\>, ...\n",
    "\n",
    "Конечная цель процедуры обучения - получить такие значения параметров модели, при которых значение функции потерь минимально.\n",
    "Иначе говоря, обучение модели ~= оптимизационная задача.\n",
    "\n",
    "В основном в \"промышленном\" (иначе говоря - коммерческом, т.е. не-академическом) ML / DL для оптимизации функций потерь используется **метод градиентного спуска** (*gradient descent*) и его вариации.\n",
    "Другие способы широкого применения по разным причинам не находят - случайный поиск, градиентные методы высшего порядка, и др. **Подумайте, почему?**\n",
    "\n",
    "В методе градиентного спуска для оптимизации целевой функции (потерь) по параметрам (модели) итеративно вычисляются ее градиенты по этим параметрам, и производится обновление весов в направлении анти-градиента: $$ w_{i+1} = w_i - \\eta \\nabla _{w} L $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13750810-4044-494b-8379-141b180b2184",
   "metadata": {
    "id": "13750810-4044-494b-8379-141b180b2184"
   },
   "source": [
    "## 3. Производные и градиенты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7186329-b4e6-4a99-a136-91806cc52dfe",
   "metadata": {
    "id": "b7186329-b4e6-4a99-a136-91806cc52dfe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594e68a-3c01-4c9e-95b9-2dacc565bd5a",
   "metadata": {
    "id": "7594e68a-3c01-4c9e-95b9-2dacc565bd5a"
   },
   "source": [
    "### 3.1. Функция одной переменной "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96d540-4140-47f1-90ee-56d6b04399b6",
   "metadata": {
    "id": "3b96d540-4140-47f1-90ee-56d6b04399b6"
   },
   "source": [
    "Возьмем простую функцию от одного аргумента:\n",
    "\n",
    "$$ f(x) = x^2 - 2x + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea37a3-8346-4502-a103-8ea954a66bf6",
   "metadata": {
    "id": "26ea37a3-8346-4502-a103-8ea954a66bf6"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2 - 2 * x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480d643-bb44-4304-9bb4-ed0af7085048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5480d643-bb44-4304-9bb4-ed0af7085048",
    "outputId": "fcfdaac6-debb-4486-9575-ddff4e57fdfe"
   },
   "outputs": [],
   "source": [
    "for x in (-1, 0, 4):\n",
    "    print(f\"x = {x:2}, f(x) = {f(x):3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1cfd3-b922-43c2-8f3a-2e0b5a3fadd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "e7e1cfd3-b922-43c2-8f3a-2e0b5a3fadd8",
    "outputId": "9dc32241-9daa-4f2b-987f-18d7cb1aeb3c"
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-2, 4, step=0.1)\n",
    "\n",
    "fs = f(xs)\n",
    "\n",
    "plt.plot(xs, fs)\n",
    "plt.grid(True)\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f865d7-c137-469f-ba61-80422849285e",
   "metadata": {
    "id": "e4f865d7-c137-469f-ba61-80422849285e"
   },
   "source": [
    "Пусть нам требуется, например, найти точки, в которых производная обращается в ноль. Все как в школе:\n",
    "\n",
    "$$ f(x) = x^2 - 2x + 1 $$\n",
    "$$ \\frac{df}{dx} = f'(x) = 0 $$\n",
    "$$ f'(x) = 2x - 2 = 2(x - 1) = 0 $$\n",
    "$$ x = 1 $$\n",
    "\n",
    "Хотелось бы проверить, что производная в полученной точке действительно равна нулю. Сделаем это численно, а для этого вспомним определение производной:\n",
    "\n",
    "$$ \\frac{df}{dx} = \\lim_{h\\rightarrow0}{\\frac{f(x+h) - f(x)}{h}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45793e9d-779a-4de3-9c7a-91dc721822a3",
   "metadata": {
    "id": "45793e9d-779a-4de3-9c7a-91dc721822a3"
   },
   "outputs": [],
   "source": [
    "def compute_derivative(func, x, h):\n",
    "    return (func(x + h) - func(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d2329-2f64-473b-94be-2afdc4fdc46e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b3d2329-2f64-473b-94be-2afdc4fdc46e",
    "outputId": "89fbf437-c75d-4c98-fe39-59bd6cf56599"
   },
   "outputs": [],
   "source": [
    "compute_derivative(f, 1, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be6588-69c5-4345-9219-ab9776d01d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57be6588-69c5-4345-9219-ab9776d01d66",
    "outputId": "5f0f9768-8410-4c35-aea7-2e23802abca3"
   },
   "outputs": [],
   "source": [
    "compute_derivative(f, 1, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cfcd8-48da-4574-8148-4f841a742bea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "893cfcd8-48da-4574-8148-4f841a742bea",
    "outputId": "c219c335-a46d-42b6-e221-c3b44e2d75cd"
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "for h in np.logspace(start=0, stop=-10, base=10, num=11):\n",
    "    print(f\"x = {x}, h = {h:6}, f'(x) = {compute_derivative(f, x, h)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157deb6a-bd5a-4558-8fab-2a9d3d05896d",
   "metadata": {
    "id": "157deb6a-bd5a-4558-8fab-2a9d3d05896d"
   },
   "source": [
    "Подсчитаем производную в другой точке, например в $x = -1$ :\n",
    "\n",
    "$$ f'(x=-1) = 2(-1-1) = -4 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5169d3f-b432-4af0-b662-e1f37fc4d6ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5169d3f-b432-4af0-b662-e1f37fc4d6ba",
    "outputId": "3b6f5fbd-f38d-41d7-c711-4d734d06d2b1"
   },
   "outputs": [],
   "source": [
    "x = -1\n",
    "for h in np.logspace(start=0, stop=-10, base=10, num=11):\n",
    "    print(f\"x = {x}, h = {h:6}, f'(x) = {compute_derivative(f, x, h)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c4229-3fe1-48cc-9d3e-18070296c3c2",
   "metadata": {
    "id": "582c4229-3fe1-48cc-9d3e-18070296c3c2"
   },
   "source": [
    "Для численной аппроксимации производной можно использовать [симметричную формулу](http://aco.ifmo.ru/el_books/numerical_methods/lectures/glava1.html), которая может работать лучше в точках, где функция \"менее гладкая\":\n",
    "$$ \\frac{df}{dx} = \\lim_{h\\rightarrow0}{\\frac{f(x+h) - f(x-h)}{2h}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfabeb-8e00-46b7-a6e0-b26b91f7fffa",
   "metadata": {
    "id": "9fbfabeb-8e00-46b7-a6e0-b26b91f7fffa"
   },
   "outputs": [],
   "source": [
    "def compute_derivative_symmetric(func, x, h):\n",
    "    return (func(x + h) - func(x - h)) / (2 * h)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62235a2d-aa28-4097-8989-c99718c04304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62235a2d-aa28-4097-8989-c99718c04304",
    "outputId": "29612675-5b8c-4101-fb15-b7a8dcae52d2"
   },
   "outputs": [],
   "source": [
    "x = -1\n",
    "for h in np.logspace(start=0, stop=-10, base=10, num=11):\n",
    "    print(f\"x = {x}, h = {h:6}, f'(x) = {compute_derivative_symmetric(f, x, h)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c602b5-bfe2-4ff3-8641-461b556a352e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50c602b5-bfe2-4ff3-8641-461b556a352e",
    "outputId": "cfd4ab75-c313-4c62-b765-f92fc9c56679"
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "for h in np.logspace(start=0, stop=-10, base=10, num=11):\n",
    "    print(f\"x = {x}, h = {h:6}, f'(x) = {compute_derivative_symmetric(f, x, h)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfdeb0-dd9e-4e05-91a8-f785d91b8b04",
   "metadata": {
    "id": "e1dfdeb0-dd9e-4e05-91a8-f785d91b8b04",
    "tags": []
   },
   "source": [
    "### 3.2. Функция нескольких переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100d9de-ad62-4ba9-90b3-eabac81eba32",
   "metadata": {
    "id": "b100d9de-ad62-4ba9-90b3-eabac81eba32"
   },
   "source": [
    "Двинемся дальше - рассмотрим функцию от двух переменных:\n",
    "\n",
    "$$ g(x, y) = 2(x^2 - 2x + 1) + (xy)^2 = 2f(x) + y^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb9003-af20-45f5-8e7d-5a17d8b69e2c",
   "metadata": {
    "id": "57cb9003-af20-45f5-8e7d-5a17d8b69e2c"
   },
   "outputs": [],
   "source": [
    "def g(x, y):\n",
    "    return 2 * (x ** 2 - 2 * x + 1) + y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b4563-293b-4934-b5c0-e0bf9ea168a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "d08b4563-293b-4934-b5c0-e0bf9ea168a6",
    "outputId": "8620a988-7db0-4212-9c28-e58cdc42c53d"
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-2, 4, step=0.01)\n",
    "ys = np.arange(-4, 4, step=0.01)\n",
    "xs, ys = np.meshgrid(xs, ys)\n",
    "\n",
    "gs = np.asarray([g(x, y) for x, y in zip(xs, ys)])\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xs, ys, gs, cmap=\"plasma\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"g(x, y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371edfc-b9e3-45ca-8260-fe6a2b2ebc86",
   "metadata": {
    "id": "d371edfc-b9e3-45ca-8260-fe6a2b2ebc86"
   },
   "source": [
    "Вспомним, что для функций нескольких переменных существует понятие частной производной (*partial derivative*). Она определяется как производная функции по заданной переменной при условии, что значения остальных переменных \"заморожены\":\n",
    "\n",
    "$$ \\frac{\\partial f(x, y, z)}{\\partial y} = f'_y = \\lim_{h\\rightarrow0}{\\frac{f(x, y + h, z) - f(x, y, z)}{h}} $$\n",
    "\n",
    "Если посчитать все частные производные в точке и собрать из них вектор, получится градиент (*gradient*):\n",
    "\n",
    "$$ \\nabla f(x, y, z) = \\{f'_x, f'_y, f'_z\\}$$\n",
    "\n",
    "**NB**: У функций одной переменной частная производная и \"обычная\" (полная) - это одно и то же, поэтому для них понятие градиента аналогично понятию производной. Вследствие этого в литературе (и жизни) понятия градиента и производных часто смешиваются. Смешивать их придется и нам :point_right: :point_left:\n",
    "\n",
    "От градиента нам нужно его главное свойство - то, что в каждой точке он указывает направление наибольшего роста функции (доказать это будет *хорошим домашним упражнением*). \n",
    "\n",
    "\n",
    "Но ненадолго забудем про градиент и вернемся к частным производным, без которых его не вычислить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e9194-bffe-400f-8f14-895ec7339995",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "635e9194-bffe-400f-8f14-895ec7339995",
    "outputId": "73805f11-fa94-45a8-9042-62a5f73a7aae"
   },
   "outputs": [],
   "source": [
    "h = 1e-9\n",
    "\n",
    "x = 1\n",
    "y = 0\n",
    "print(f\"partial dg/dx (x={x}, y={y}) = {(g(x + h, y) - g(x, y)) / h}\") \n",
    "print(f\"partial dg/dy (x={x}, y={y}) = {(g(x, y + h) - g(x, y)) / h}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a896b9-1cba-45eb-979c-822952b865ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69a896b9-1cba-45eb-979c-822952b865ac",
    "outputId": "7c6d9a25-7f08-4747-849b-48d0379eaabc"
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = 2\n",
    "print(f\"partial dg/dx (x={x}, y={y}) = {(g(x + h, y) - g(x, y)) / h}\") \n",
    "print(f\"partial dg/dy (x={x}, y={y}) = {(g(x, y + h) - g(x, y)) / h}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f45d0-4567-400d-916b-2debe6874f3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c9f45d0-4567-400d-916b-2debe6874f3a",
    "outputId": "68bd2063-b5c8-4b5b-8fd2-f39b1ab540ad"
   },
   "outputs": [],
   "source": [
    "x = -1\n",
    "y = 0\n",
    "print(f\"partial dg/dx (x={x}, y={y}) = {(g(x + h, y) - g(x, y)) / h}\") \n",
    "print(f\"partial dg/dy (x={x}, y={y}) = {(g(x, y + h) - g(x, y)) / h}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83970a57-c541-4f83-a34f-7ba2b52538e2",
   "metadata": {
    "id": "83970a57-c541-4f83-a34f-7ba2b52538e2"
   },
   "source": [
    "Посчитаем также частные производные $ g(x, y) $ вручную. Можно рассматривать $ g $ только как функцию от $x, y$:\n",
    "\n",
    "$$ g(x, y) = 2(x^2 - 2x + 1) + (xy)^2 = 2x^2 - 4x + 2 + x^2y^2 $$ \n",
    "$$ \\frac{\\partial g}{\\partial x} = 4x - 4 + 2xy^2 = 2(2x + xy^2 - 2) $$ \n",
    "$$ \\frac{\\partial g}{\\partial y} = 2x^2y $$ \n",
    "\n",
    "С другой стороны, мы специально ввели функцию $g$ таким образом, чтобы она была выразима через другую функцию, $f$, рассмотренную раньше:\n",
    "$$ g(x, y) = 2(x^2 - 2x + 1) + x^2y^2 = 2f(x) + x^2y^2 $$\n",
    "\n",
    "Поэтому взятие производной можно произвести и другим способом, как производную сложной функции:\n",
    "$$ \\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\frac{\\partial f}{\\partial x} + \\frac{\\partial(x^2y^2)}{\\partial x}  $$\n",
    "$$ \\frac{\\partial g}{\\partial f} = 2 $$\n",
    "$$ \\frac{\\partial f}{\\partial x} = 2(x - 1) $$\n",
    "$$ \\frac{\\partial g}{\\partial x} = 2 * 2(x - 1) + 2xy^2 = 2(2x + xy^2 - 2) $$\n",
    "$$ \\frac{\\partial g}{\\partial y} = 2x^2y $$ \n",
    "\n",
    "Результат получился таким же, но действий на первый взгляд пришлось сделать больше. Так зачем усложнять?\n",
    "\n",
    "Представим, что нам опять принесли новую функцию $u$ и просят продифференцировать ее по всем аргументам: \n",
    "$$ u(x, y, z) = 10g(x, y) - 100500z $$ \n",
    "\n",
    "Можно подставить выражение для $g(x,y)$, в него подставить выражение для $f(x, y)$, и посчитать все в лоб:\n",
    "$$ НЕТ $$\n",
    "\n",
    "Ведь мы же уже знаем частные производные $g$, зачем добру пропадать? В этом случае ощутимо легче продифференцировать $u$ по $g$ и подставить известные значения $g'_{x}$ и $g'_{y}$:\n",
    "\n",
    "$$ \\frac{\\partial u}{\\partial g} = 10 $$\n",
    "$$ \\frac{\\partial u}{\\partial x} = \\frac{\\partial u}{\\partial g} \\frac{\\partial g}{\\partial x} = 10 * 2(2x + xy^2 - 2) $$\n",
    "$$ \\frac{\\partial u}{\\partial y} = \\frac{\\partial u}{\\partial g} \\frac{\\partial g}{\\partial y} = 10 * 2x^2y $$\n",
    "$$ \\frac{\\partial u}{\\partial z} = -100500 $$\n",
    "\n",
    "Таким образом, если мы работаем со сложными функциями (причем \"сильно вложенными\", $f(x) = f_n(f_{n-1}(f_{n-2}(...f_1(x_1, ..., x_m))$), то можно вычислять производные последовательно, переиспользуя уже совершённые вычисления на каждом шаге.\n",
    "\n",
    "* Мы говорим: **\"сложная производная\"**\n",
    "* Они говорят: **\"chain rule\"**\n",
    "\n",
    "**Спойлер**: нейронные сети как раз такие.\n",
    "\n",
    "![resnet](https://www.researchgate.net/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture.png)\n",
    "\n",
    "(ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb9344-1303-40b5-8d00-593ff0fbfd96",
   "metadata": {
    "id": "6cbb9344-1303-40b5-8d00-593ff0fbfd96"
   },
   "source": [
    "### 3.3. Вычислительные графы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d7a2b-8e53-4fc8-9b22-9fac7e023c62",
   "metadata": {
    "id": "8a2d7a2b-8e53-4fc8-9b22-9fac7e023c62"
   },
   "source": [
    "Добавим еще одну точку зрения на алгебраические выражения. Возьмем такое: $ f(x, y) = 3x - y + 1 $.\n",
    "\n",
    "Представим, что мы компьютер, и нам нужно вычислить значение этого выражения. Наши действия:\n",
    "* Взять $x$, умножить на 3\n",
    "* Взять $y$, умножить на -1\n",
    "* Сложить предыдущие два результата\n",
    "* Сложить предыдущий результат с 1\n",
    "\n",
    "Визуализируем этот алгоритм:\n",
    "\n",
    "![comp_graph_01](https://i.ibb.co/xCDh23Z/aim-seminar01-compgraphs-5.png)\n",
    "\n",
    "Получился - :drums: - вычислительный граф:\n",
    "* Ориентированный\n",
    "    * Направление ребер соответствует \"движению\" от входов (аргументов) к выходам (итоговому значению) \n",
    "* Узлы соответствуют операциям\n",
    "    * Входы - переменные (аргументы / параметры), \n",
    "    * Выходы - результат вычисления\n",
    "\n",
    "![comp_graph_01_fwd](https://i.ibb.co/DRqMNNV/aim-seminar01-compgraphs-6.png)\n",
    "    \n",
    "По графу можно отследить путь от \"конца\" (значения выражения) к \"началу\" (конкретному входному аргументу выражения). Нам это пригодится для того, чтобы вычислять производные всего выражения по промежуточным переменным и параметрам выражения:\n",
    "\n",
    "![comp_graph_01_bwd](https://i.ibb.co/FzpKH9K/aim-seminar01-compgraphs-7.png)\n",
    "\n",
    "Еще пример: $ f(x, y) = 3x - xy + 1 $.\n",
    "\n",
    "![comp_graph_02](https://i.ibb.co/3cV2F9f/aim-seminar01-compgraphs-8.png)\n",
    "\n",
    "![comp_graph_02_fwd](https://i.ibb.co/7QDzDKr/aim-seminar01-compgraphs-9.png)\n",
    "\n",
    "![comp_graph_02_bwd](https://i.ibb.co/WPWFtPy/aim-seminar01-compgraphs-10-2.png)\n",
    "\n",
    "\n",
    "**Важный итог: вычисление \"длинных\" производных сводится к вычислению \"локальных\" в узлах графа.**\n",
    "\n",
    "\n",
    "**NB**: Обычно в задачах ML речь идет об оптимизации функции потерь по параметрам модели, а не по входным данным. Т.е. функцию $f(x, y)$, будь она нашей \"моделью\", мы бы оптимизировали по ее параметрам $\\{w_i\\}$: $f(x, y|w_1, w_2, w_3) = w_1x + w_2y + w_3$\n",
    "\n",
    "*(Бывают и исключения: например, задача Style Transfer, в которой оптимизируется именно входное изображение, т.е. \"$x$\". Об этом мы поговорим на 10 семинаре курса.)*\n",
    "\n",
    "![comp_graph_03](https://i.ibb.co/f8yv1Kc/aim-seminar01-compgraphs-11-1.png)\n",
    "\n",
    "Каверзные вопросы:\n",
    "* Требуется ли для вычисления $f'_{w_1}$ вычислять $f'_{w_2}$?\n",
    "* Требуется ли для вычисления $f'_{w_1}$ вычислять $f'_{w_3}$?\n",
    "* Требуется ли для вычисления $f'_{w_1}$ вычислять $f'_{a}$?\n",
    "* Требуется ли для вычисления $f'_{x}$ вычислять $f'_{w_2}$?\n",
    "* Требуется ли для вычисления $f'_{x}$ вычислять $f'_{w_1}$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BFTHNtsx_aTn",
   "metadata": {
    "id": "BFTHNtsx_aTn"
   },
   "source": [
    "Маленький тизер возможностей библиотеки Pytorch:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a926de5-8698-489a-a796-0ce9e9e26528",
   "metadata": {
    "id": "85d8ef50-aab9-43c7-8aef-946359b86cf4"
   },
   "source": [
    "!pip install torch > /dev/null\n",
    "!pip install torchviz > /dev/null"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fff92e9-fa44-462d-9570-ddf26e7287eb",
   "metadata": {
    "id": "af4a9033-03b2-49ec-84ab-5506a92e2eb7"
   },
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6ae1771-ca61-49da-b14f-0880a5e46ca1",
   "metadata": {
    "id": "4AEYxGEKquD2"
   },
   "source": [
    "def f(x, y, w1, w2, w3):\n",
    "    return w1 * x + w2 * x * y + w3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "352b7259-86ca-41ac-bd0a-5ce3dfe7f4ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05301d0c-777e-49d5-9d75-9c009fd7c2e1",
    "outputId": "9c348f94-f129-4474-fa33-3c860d17328e"
   },
   "source": [
    "# переменные\n",
    "x = Variable(Tensor([1]), requires_grad = True)\n",
    "y = Variable(Tensor([2]), requires_grad = True)\n",
    "w1 = Variable(Tensor([3]), requires_grad = False)\n",
    "w2 = Variable(Tensor([-1]), requires_grad = False)\n",
    "w3 = Variable(Tensor([1]), requires_grad = False)\n",
    "\n",
    "out = f(x, y, w1, w2, w3) # прямой проход - вычисление\n",
    "out.backward() # обратный проход - выч. производных\n",
    "print(x.grad, y.grad) # производные"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de5b0ba7-da46-48e0-8e18-977e7c830670",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "n76VSgaaeiPs",
    "outputId": "91f80627-3dab-4db3-aa34-b38f67ef1457"
   },
   "source": [
    "make_dot(out, params={\"x\": x, \"y\": y, \"w1\": w1, \"w2\": w2, \"w3\": w3, \"out\": out})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D75HoYXoAB3Y",
   "metadata": {
    "id": "D75HoYXoAB3Y"
   },
   "source": [
    "### 3.4. \"Боевой\" пример"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nj4Fjd2OASN7",
   "metadata": {
    "id": "Nj4Fjd2OASN7"
   },
   "source": [
    "Пусть нам нужно решить задачу регрессии в плоскости.\n",
    "Данные имеют следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ThifZQ0Q_jL-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ThifZQ0Q_jL-",
    "outputId": "08d00246-1147-4846-e8cc-4ec56aab1b89"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_a = np.random.uniform(1, 5)\n",
    "_b = np.random.uniform(-3, 3)\n",
    "_c = np.random.uniform(-3, 3)\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "xs = np.random.uniform(-3, 3, size=num_samples)\n",
    "ys_clean = _a * xs ** 2 + _b * xs + _c\n",
    "ys_noise = np.random.normal(0, 1, size=len(ys_clean))\n",
    "ys = ys_clean + ys_noise\n",
    "\n",
    "plt.scatter(xs, ys, label=\"gt\", s=5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pS1drHphBtER",
   "metadata": {
    "id": "pS1drHphBtER"
   },
   "source": [
    "Выберем квадратичную модель: $f(x) = ax^2 + bx + c$.\n",
    "Фактически, нам нужно \"угадать\" по данным неизвестные параметры `a`, `b` и `c`.\n",
    "Подбирать их будем итеративно, используя:\n",
    "  * В качестве алгоритма оптимизации - стохастический градиентный спуск (`SGD`)\n",
    "    * $ w_{i+1} = w_i - \\eta \\nabla L _{w} $\n",
    "  * В качестве функции потерь - среднеквадратичную ошибку `MSE`\n",
    "    * $ L = ||y - \\hat{y}||^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "goKLzO3WFD0t",
   "metadata": {
    "id": "goKLzO3WFD0t"
   },
   "outputs": [],
   "source": [
    "a = 1.   # Initial values\n",
    "b = 1.   # ...\n",
    "c = 1.   # ...\n",
    "\n",
    "def model(x, a, b, c):\n",
    "    return a * x ** 2 + b * x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qxer39rqLVxI",
   "metadata": {
    "id": "qxer39rqLVxI"
   },
   "source": [
    "Построим граф для вычисления нашей модельной функции:\n",
    "\n",
    "![comp_graph_03](https://i.ibb.co/Jvcvx7Z/aim-seminar01-compgraphs-12.png)\n",
    "\n",
    "Возьмем 1 точку из датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhgMTgnXA8FK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhgMTgnXA8FK",
    "outputId": "1a23733e-e8f7-49bc-dd0f-41747a0c0dff"
   },
   "outputs": [],
   "source": [
    "i = 25\n",
    "x = xs[i]\n",
    "y = ys[i]\n",
    "\n",
    "print(x)  # ~ -1.1\n",
    "print(y)  # ~ 0.41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lwgtZ73ALbBN",
   "metadata": {
    "id": "lwgtZ73ALbBN"
   },
   "source": [
    "Узнаем, что предсказывает модель в текущем состоянии в этой точке:\n",
    "\n",
    "![comp_graph_03_fwd](https://i.ibb.co/B3GQmHW/aim-seminar01-completed-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gOeUMu2oFcWk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOeUMu2oFcWk",
    "outputId": "1f57513c-6fa9-4fcd-c702-534c10355b7b"
   },
   "outputs": [],
   "source": [
    "y_hat = model(x, a, b, c)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FHqXV53MMdDY",
   "metadata": {
    "id": "FHqXV53MMdDY"
   },
   "source": [
    "Дальше по программе нужно запустить вычисление градиентов от конца (`L`) до нужных нам переменных (весов `a`, `b` и `c`). Это и есть **Backpropagation**:\n",
    "\n",
    "![comp_graph_03_bwd](https://i.ibb.co/9nxNrsh/aim-seminar01-compgraphs-14.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8uoS2_vFgPy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8uoS2_vFgPy",
    "outputId": "489d43f4-a0fc-477c-9d52-a5e11f0ab362"
   },
   "outputs": [],
   "source": [
    "a_t = Variable(Tensor([a]), requires_grad=True)\n",
    "b_t = Variable(Tensor([b]), requires_grad=True)\n",
    "c_t = Variable(Tensor([c]), requires_grad=True)\n",
    "y_hat = model(x, a_t, b_t, c_t)\n",
    "loss = (y - y_hat) ** 2\n",
    "loss.backward()\n",
    "\n",
    "print(y, y_hat, loss)\n",
    "print(a_t.grad)\n",
    "print(b_t.grad)\n",
    "print(c_t.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9YZk67tsVK1a",
   "metadata": {
    "id": "9YZk67tsVK1a"
   },
   "source": [
    "Далее обновляем значения весов `a`, `b` и `c` в соответствии с формулой для SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vmGZ_2LVVY6L",
   "metadata": {
    "id": "vmGZ_2LVVY6L"
   },
   "outputs": [],
   "source": [
    "lr = 0.1   # Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LZEKEE0pVVfG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZEKEE0pVVfG",
    "outputId": "71e12521-cc7d-4882-d8ab-73dbbb53ccbc"
   },
   "outputs": [],
   "source": [
    "a = a - lr * a_t.grad\n",
    "b = b - lr * b_t.grad\n",
    "c = c - lr * c_t.grad\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aFFIinyIVz4p",
   "metadata": {
    "id": "aFFIinyIVz4p"
   },
   "source": [
    "Всю это последовательность шагов нужно повторить для всех объектов обучающей выборке, возможно - не один раз:\n",
    "  * Вычисление $ \\hat{y} $, \n",
    "  * Вычисление $ L(y, \\hat{y}) $, \n",
    "  * Вычисление $ \\nabla L_{\\{w\\}} $\n",
    "  * Обновление $ \\{w\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BOXl2HFFWzao",
   "metadata": {
    "id": "BOXl2HFFWzao"
   },
   "source": [
    "\n",
    "```python\n",
    "for epoch in num_epochs:\n",
    "  for (x, y) in zip(xs, ys):\n",
    "    y_hat = model(x, a, b, c)\n",
    "    \n",
    "    loss = (y - y_hat) ** 2\n",
    "    loss.backward()\n",
    "\n",
    "    a = a - lr * a.grad\n",
    "    b = b - lr * b.grad\n",
    "    c = c - lr * c.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xnWbAEK6X4mu",
   "metadata": {
    "id": "xnWbAEK6X4mu"
   },
   "source": [
    "## 4. Векторы и матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vNkHt55XmYXl",
   "metadata": {
    "id": "vNkHt55XmYXl"
   },
   "source": [
    "До этого момента мы работали с одномерными примерами - вещественные числа на входе и выходе каждой из операций. Но и в \"классическом\" ML, и в DL (то бишь Deep Learning) обычно работают с многомерными данными (векторами признаков, тензорами изображений, ...). А это значит, что и операции над ними становятся \"многомерными\". Поскольку мы хотим использовать BackProp для обучения моделей и с такими операциями тоже, нам нужно уметь считать для них градиенты.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DDAJ-6ECneOO",
   "metadata": {
    "id": "DDAJ-6ECneOO"
   },
   "source": [
    "### 4.1. Скалярное произведение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77Pyej3RnkGz",
   "metadata": {
    "id": "77Pyej3RnkGz"
   },
   "source": [
    "Пусть имеется вектор $ x $: $$ x = [x_1, ..., x_n]^T \\in R^n $$\n",
    "Пусть также имеется вектор параметров $ a $:  $$ a = [a_1, ..., a_n]^T \\in R^n $$\n",
    "\n",
    "Пусть в неком слое выполняется операция скалярного произведения:  $$ f(x, a) = a \\bullet x \\in R $$\n",
    "\n",
    "Надо найти градиент этой функции по $ x $.\n",
    "\n",
    "* Какую размерность будет иметь $ \\nabla_{x} f $?\n",
    "* Как собственно считать?\n",
    "\n",
    "\n",
    "С размерностями градиентов есть хорошее правило. Поскольку градиент - это набор частных производных, то нам нужно найти частные производные всех выходов по всем входам. Посмотрите, сколько элементов у результата операции (выхода), и сколько элементов у аргумента (входа), по которому надо посчитать градиент. \n",
    "\n",
    "У нас:\n",
    "* На выходе 1 число\n",
    "* На входе вектор длины $ n $ ($ x $ или $ a $).\n",
    "\n",
    "Значит, в градиенте должно быть $ n $ чисел, т.е. должен получиться снова вектор.\n",
    "\n",
    "Осталось его посчитать.\n",
    "\n",
    "![deriv_dotp](https://i.ibb.co/Prtx4xs/aim-seminar01-compgraphs-11-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YtgTUN2nuY4_",
   "metadata": {
    "id": "YtgTUN2nuY4_"
   },
   "source": [
    "$ \\nabla_a f $ посчитайте сами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kVsOx686utQC",
   "metadata": {
    "id": "kVsOx686utQC"
   },
   "source": [
    "### 4.2. Линейный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_p4L-HGKux4w",
   "metadata": {
    "id": "_p4L-HGKux4w"
   },
   "source": [
    "Теперь умножим на наш вектор $ x $ матрицу $ A \\in R^{m \\times n} $: \n",
    "$$ \\begin{pmatrix}\n",
    "a_{11} & ... & a_{1n} \\\\\\ \n",
    "...    & ... & ...    \\\\\\ \n",
    "a_{m1} & ... & a_{mn}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "* Найти градиент $ f(x) = Ax $ по $ x $\n",
    "\n",
    "Применим снова прием с размерностями. На выходе вектор длины $ m $, на входе вектор длины $ n $. Считать будем частные производные \"всех по всем\", значит, размерность градиента будет $ m \\times n $. Считаем:\n",
    "\n",
    "![deriv_linear](https://i.ibb.co/kVn77pW/aim-seminar01-compgraphs-14-1.png)\n",
    "\n",
    "\n",
    "* Найти градиент $ f(x) = Ax $ по $ A $\n",
    "\n",
    "Вы уже догадались, что будете делать это сами?\n",
    "\n",
    "Что еще посчитать (по всем входам):\n",
    "\n",
    "* $ f(x) = \\alpha x $ (умножение вектора на скаляр)\n",
    "* $ f(x) = x^{T}A; $\n",
    "* $ f(X) = Tr(X) $\n",
    "* $ f(X) = X^T$\n",
    "* $ f(X) = AX $ (умножение матрицы на матрицу)\n",
    "\n",
    "\n",
    "И из нелинейностей:\n",
    "\n",
    "* $ f(X) = ReLU(X) $\n",
    "* $ f(x) = \\sigma (x) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIznFY_66Bjh",
   "metadata": {
    "id": "jIznFY_66Bjh"
   },
   "source": [
    "Что еще почитать:\n",
    "* [Хорошая методичка от cs231n](http://cs231n.stanford.edu/vecDerivs.pdf)\n",
    "* [Конспект от cs231n](https://cs231n.github.io/optimization-2/)\n",
    "* [Возможно, лучшее в мире объяснение BackProp от A.Karpathy](https://www.youtube.com/watch?v=VMj-3S1tku0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yo3AdaoD6sLy",
   "metadata": {
    "id": "yo3AdaoD6sLy"
   },
   "source": [
    "В следующий раз:\n",
    "* Библиотека PyTorch\n",
    "* Автоматическое дифференцирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XSVy8-fYX63w",
   "metadata": {
    "id": "XSVy8-fYX63w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
