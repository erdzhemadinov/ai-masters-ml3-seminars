{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dceb3976-ff98-4d4b-b7c6-fb7f98780c28",
   "metadata": {},
   "source": [
    "# 09. Сегментация + UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648decf5-d311-4693-97d2-3d309984e10b",
   "metadata": {},
   "source": [
    "Сегодня научимся сегментировать изображения на примере задачи отделения людей от заднего фона. В качестве модели будем использовать (сюрприз-сюрприз) UNet-подобную сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ec39c-c91a-417b-848c-e0cea154c754",
   "metadata": {},
   "source": [
    "## План:\n",
    "\n",
    "1. Данные для сегментации\n",
    "2. UNet: \n",
    "    * Encoder\n",
    "    * Decoder\n",
    "    * Все вместе\n",
    "3. Dice / Dice Loss\n",
    "4. *UNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18d427-30ad-43c3-a282-ed45fbab5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ac95a-0f01-4b23-8936-51c11b201e84",
   "metadata": {},
   "source": [
    "### 1. Данные для сегментации\n",
    "\n",
    "Будем использовать данные с первого этапа **PicsArt AI Hackathon**. Скачать [тут](https://drive.google.com/file/d/1Zc8iMETenxtk10GQEeNyMDY5PST7izjv/view?usp=sharing).\n",
    "- Загрузим и распакуем данные\n",
    "- Напишем класс `Dataset` для наших данных\n",
    "- Посмотрим примеры данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eea975-91b5-4ebd-afcb-ea18643bb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanSegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, split=\"train\", transform=None):\n",
    "        \n",
    "        image_filenames = sorted(glob.glob(os.path.join(root, split, \"*.jpg\")))\n",
    "        mask_filenames = []\n",
    "        \n",
    "        if split != \"test\":\n",
    "            mask_filenames = []\n",
    "            for image_fn in image_filenames:\n",
    "                mask_fn = image_fn.replace(split, f\"{split}_mask\").replace(\"jpg\", \"png\")\n",
    "                if not os.path.exists(mask_fn):\n",
    "                    raise FileNotFoundError(mask_fn)\n",
    "                mask_filenames.append(mask_fn)\n",
    "                \n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # write some code here\n",
    "        image = cv2.imread(self.image_filenames[idx]).astype(np.float32) / 255.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = None\n",
    "        if self.split != \"test\":\n",
    "            mask = cv2.imread(self.mask_filenames[idx], cv2.IMREAD_GRAYSCALE) / 255\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            \n",
    "        image = transformed[\"image\"]\n",
    "        mask = transformed[\"mask\"]\n",
    "        mask = mask[None, ...]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5c23f-3ebf-48aa-9fac-c3c373194255",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad185d-7d53-44f4-bc34-26a24f0a9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HumanSegmentationDataset(root=\"data\", transform=transform)\n",
    "valid_dataset = HumanSegmentationDataset(root=\"data\", transform=transform, split=\"valid\")\n",
    "test_dataset = HumanSegmentationDataset(root=\"data\", transform=transform, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c159f-789d-4d97-81c4-8788ff73a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a32d9-e0e1-40ee-8c52-3f74cc5db3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output\n",
    "image.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77fafd-855b-4180-977c-94bd545c37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_mask(image, mask, predicted=None):\n",
    "    \n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    mask = (mask >= 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(1, 6, 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(1, 6, 3)\n",
    "    image_masked = cv2.bitwise_and(image, image, mask=mask)\n",
    "    plt.imshow(image_masked)\n",
    "    plt.axis(False)\n",
    "\n",
    "    if predicted is not None:\n",
    "        predicted = (predicted * 255).astype(np.uint8)\n",
    "        plt.subplot(1, 6, 4)\n",
    "        plt.imshow(predicted)\n",
    "        plt.axis(False)\n",
    "        \n",
    "        predicted_binary = (predicted >= 0.5 * 255).astype(np.uint8) * 255\n",
    "        plt.subplot(1, 6, 5)\n",
    "        plt.imshow(predicted_binary)\n",
    "        plt.axis(False)\n",
    "    \n",
    "        image_masked = cv2.bitwise_and(image, image, mask=predicted_binary)\n",
    "        plt.subplot(1, 6, 6)\n",
    "        plt.imshow(image_masked)\n",
    "        plt.axis(False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de0987-d495-47ef-b0e5-00f89270bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.randint(0, len(train_dataset), 4)\n",
    "\n",
    "sample_dataset = [train_dataset[i] for i in idxs]\n",
    "sample_dataset = [(image.numpy().transpose(1, 2, 0), mask.numpy()[0]) for (image, mask) in sample_dataset]\n",
    "\n",
    "for image, mask in sample_dataset:\n",
    "    show_image_with_mask(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c1bb7-af78-4027-b458-9e8e33293052",
   "metadata": {},
   "source": [
    "### 2. Собираем UNet\n",
    "\n",
    "![unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a60b03-354e-475b-9db2-a2958e640256",
   "metadata": {},
   "source": [
    "#### 2.1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b393cc-5944-4f14-8e7b-30898a43248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c698c1-6132-41ff-b83d-63fba8809038",
   "metadata": {},
   "source": [
    "Один **блок кодировщика** состоит из двух сверток, активаций и опционального батчнорма. **Блок не изменяет размеров HxW** входного тензора (это будем делать снаружи):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77952052-8836-4c09-8cc7-cb619e78544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        \n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        \n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ac29a-9ba2-4bd8-8c2c-6427f2a6f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = EncoderBlock(3, 16)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d9a71-88ff-4140-a478-d9fca8d54fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4, 3, 128, 128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20d807-7f5f-45c6-b621-52d9148614c9",
   "metadata": {},
   "source": [
    "Напишем класс энкодера, который и будет состоять из блоков.\n",
    "\n",
    "**Задача 1**: реализовать конструктор класса `Encoder`.\n",
    "* `num_filters` - это характерный размер блоков сети (\"ширина\"). В первом блоке должно быть `num_filters` признаков, во втором - вдвое больше, и т.д.\n",
    "* `num_blocks` - количество блоков, после каждого блока стоит MaxPool2d (кроме последнего блока).\n",
    "\n",
    "\n",
    "**Задача 2**: реализовать метод `forward` класса `Encoder`.\n",
    "* Должен вернуть список с активациями каждого блока энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66062991-4f2a-40e3-af54-833e5177e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # blocks = ...\n",
    "        # ...\n",
    "            \n",
    "        # END OF YOUR CODE\n",
    "            \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # for ...\n",
    "        # ...\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f450ec0-f449-401f-a950-2b3030e00561",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=16, num_blocks=4)\n",
    "\n",
    "assert len(encoder.blocks) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3a518-1d3e-4189-8c62-43cc4a32e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4, 3, 512, 512)\n",
    "y = encoder(x)\n",
    "\n",
    "assert len(y) == 4\n",
    "\n",
    "assert y[0].size() == (4, 16, 512, 512)\n",
    "assert y[1].size() == (4, 32, 256, 256)\n",
    "assert y[2].size() == (4, 64, 128, 128)\n",
    "assert y[3].size() == (4, 128, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e7eb9-7091-4ddc-9140-94b85865c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ef798-69f1-49fb-9a77-ae9327b03ec8",
   "metadata": {},
   "source": [
    "#### 2.2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257a9d9-7b06-4ffd-b0f1-d75b390a159d",
   "metadata": {},
   "source": [
    "С декодером несколько интереснее, каждый блок получает на вход 2 тензора, один \"слева\" (из ветки энкодера) и один \"снизу\" (из предыдушего блока декодера). Далее \"нижний\" тензор увеличивается в размерах (HxW) вдвое, конкатенируется с \"левым\" и сворачивается. Мы будем использовать самый простой вариант **билинейной интерполяции**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02ee17-6f50-4c47-85ca-04c64ca5c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.uppool = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upconv = conv3x3(out_channels*2, out_channels)\n",
    "        self.conv1 = conv3x3(out_channels*2, out_channels)\n",
    "\n",
    "    def forward(self, down, left):\n",
    "        x = self.uppool(down)\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([x, left], 1)\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85161a24-f946-4940-996f-cacb60268236",
   "metadata": {},
   "source": [
    "![interp](https://matplotlib.org/stable/_images/sphx_glr_interpolation_methods_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f6db6-083d-496a-b8cf-7aac5a481658",
   "metadata": {},
   "source": [
    "Напишем теперь и декодер:\n",
    "\n",
    "**Задача 3**: реализовать конструктор класса `Decoder`.\n",
    "* `num_filters` - это характерный размер блоков сети (\"ширина\"). В первом блоке должно быть `num_filters` признаков, во втором - вдвое больше, и т.д.\n",
    "* `num_blocks` - количество блоков, после каждого блока стоит MaxPool2d (кроме последнего блока).\n",
    "\n",
    "\n",
    "**Задача 4**: реализовать метод `forward` класса `Decoder`.\n",
    "* Должен получить активации из энкодера и вернуть один тензор\n",
    "\n",
    "**NB**: Обратите внимание, что для корректной \"склейки\" у энкодера должно быть на 1 блок больше, чем у декодера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df2f2-e442-442a-87a8-7f9db0d775c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # blocks = ...\n",
    "        \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "\n",
    "    def forward(self, acts):\n",
    "        \n",
    "        up = acts[-1]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # for ...\n",
    "        # ...\n",
    "            \n",
    "        # END OF YOUR CODE\n",
    "            \n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646cc9f-b17a-4b68-9707-462a8ac441d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(16, 3)\n",
    "\n",
    "assert len(decoder.blocks) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4af410-39ec-4fa0-a079-5bfa9f7778e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dec = decoder(y)\n",
    "\n",
    "assert y_dec.size() == (4, 16, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743be33-7540-4e6d-8552-764bb02ea0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae8070-aa8e-454d-9cb7-29a4e08700b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(encoder(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf006e6-8fb4-475f-a0c3-af31c0292fd2",
   "metadata": {},
   "source": [
    "### 2.3. UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005ac72-2d84-478e-86f8-2e1081cfd689",
   "metadata": {},
   "source": [
    "Осталось соединить энкодер с декодером и добавить вишенку на торте - выходную свертку в нужное число классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe3e54-3f4b-4034-886f-35c8a1e5f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=16, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n",
    "        self.decoder = Decoder(num_filters, num_blocks-1)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31e3fc-07bf-4b63-a366-149704d90de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_classes=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bae149-eb6f-4ec6-9cad-9a8736232f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(4, 3, 416, 416).to(device)\n",
    "\n",
    "model(images).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ef453-8fac-476b-a6d8-21fa269679c0",
   "metadata": {},
   "source": [
    "## 3. Dice / Dice Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c2dc3-1e32-40a4-b42b-6c19aa85998e",
   "metadata": {},
   "source": [
    "В базовом случае для обучения семантической сегментации используют попиксельный BCE. Для отслеживания качества сегментации можно использовать IoU для масок или его непрерывную аппроксимацию - Dice coeff:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b6b15-05da-4f0f-bcd5-e430c53b351f",
   "metadata": {},
   "source": [
    "![dice](https://miro.medium.com/max/4800/1*oK8npb1wtF-GKeHi7yIBoQ.png)\n",
    "\n",
    "![dice](https://miro.medium.com/max/514/1*EF3VCtk-VbTIKhriaQF0YQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e324b7-d7d3-49bd-91b0-43f04a957083",
   "metadata": {},
   "source": [
    "**Задача 5**: Реализуйте функцию для вычисления Dice Coeff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e904c5-d243-45fe-b3c7-f1219b5a4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(pred_tensor, target_tensor):\n",
    "    smooth = 0.1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # ...\n",
    "    # intersection = ...\n",
    "    # union = ...\n",
    "    # ...\n",
    "    \n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return 2 * (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecba919-fb78-45bd-8756-061db9c36117",
   "metadata": {},
   "source": [
    "Время обучать!\n",
    "\n",
    "В рамках семинара просто посмотрим, что модель переобучается под данные. В качестве трейна и теста будем использовать тестовый датасет. Но вы, конечно, так не делайте и попробуйте поучить на настоящем трейне, а потеститься на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f6ff4-9206-442c-a425-8167944fe133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    for each batch \n",
    "    performs forward and backward pass and parameters update \n",
    "    \n",
    "    Input:\n",
    "    model: instance of model (example defined above)\n",
    "    optimizer: instance of optimizer (defined above)\n",
    "    train_loader: instance of DataLoader\n",
    "    \n",
    "    Returns:\n",
    "    nothing\n",
    "    \n",
    "    Do not forget to set net to train mode!\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for batch_train, batch_answers in train_loader:\n",
    "        batch_train = batch_train.to(device)\n",
    "        batch_answers = batch_answers.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model_answers = model(batch_train)\n",
    "        \n",
    "        new_loss = criterion(model_answers, batch_answers)\n",
    "        new_loss.backward()\n",
    "        optimizer.step()      \n",
    "\n",
    "def evaluate_loss(loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates loss and accuracy on the whole dataset\n",
    "    \n",
    "    Input:\n",
    "    loader:  instance of DataLoader\n",
    "    model: instance of model (examle defined above)\n",
    "    \n",
    "    Returns: (loss, accuracy)\n",
    "    \n",
    "    Do not forget to set net to eval mode!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_test, batch_answers in loader:\n",
    "            batch_test = batch_test.to(device)\n",
    "            batch_answers = batch_answers.to(device)\n",
    "            \n",
    "            model_answers = torch.sigmoid(model(batch_test))\n",
    "            one_batch_loss = float(criterion(model_answers, batch_answers))\n",
    "            one_batch_dice = float(dice_coeff(model_answers.cpu().detach(), batch_answers.cpu().detach()))\n",
    "            \n",
    "            total_loss += one_batch_loss\n",
    "            total_dice += one_batch_dice\n",
    "            total_n += 1\n",
    "    \n",
    "    return (total_loss / total_n, total_dice / total_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8ea64-75da-40cc-850b-d18edb975fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d213f-a2e7-4a7b-a153-b92432e5fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_classes=1, num_filters=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "test_images, test_masks = next(iter(test_loader))\n",
    "\n",
    "for epoch in range(0, 192):\n",
    "    train_epoch(model, optimizer, test_loader, criterion, device=device)\n",
    "\n",
    "    train_loss, train_dice = evaluate_loss(test_loader, model, criterion, device=device)\n",
    "    #your code here    \n",
    "    preds = torch.sigmoid(model(test_images.to(device))).cpu().detach()\n",
    "    print(f\"Epoch {epoch}: avg train loss {train_loss:.3f}, avg dice {train_dice:.3f}\")\n",
    "\n",
    "    if epoch % 16 == 0:\n",
    "        for image, mask, pred in zip(test_images[:4], test_masks[:4], preds):\n",
    "            show_image_with_mask(image.numpy().transpose(1, 2, 0), mask.numpy()[0], pred.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddceff1-91bf-4a21-90fa-bdf079b7a9bf",
   "metadata": {},
   "source": [
    "Какие проблемы вы видите при обучении? Как можно улучшить качество?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483de4fd-d4aa-49e5-bc15-470256c7b469",
   "metadata": {},
   "source": [
    "Dice можно использовать и в качестве целевой функции - для этого надо превратить его в лосс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdc6b0-2cad-4c36-9a16-0bc07ec10a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred_tensor, target_tensor):\n",
    "        \n",
    "    loss = 1 - dice_coeff(pred_tensor, target_tensor)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e5f9b-7de5-4b1a-bbae-a2b9c757a442",
   "metadata": {},
   "source": [
    "Эксперимент с обучением на таком лоссе сделайте сами. Что получится?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fba30-e692-48c7-81b3-bb73506276f7",
   "metadata": {},
   "source": [
    "## UNet34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae64c7-c8fb-4105-bb97-cff800955afd",
   "metadata": {},
   "source": [
    "Хочется (как всегда) использовать мощь предобученных моделей. Но где взять такое для задачи сегментации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f555232-bca4-4a41-834b-546691da7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166973d6-d632-4566-a5bf-25c8b0fc91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = resnet34(pretrained=True)\n",
    "        \n",
    "        self.pre = nn.Sequential(*[model.conv1, model.bn1, model.relu, model.maxpool])\n",
    "        self.layer1 = model.layer1\n",
    "        self.layer2 = model.layer2\n",
    "        self.layer3 = model.layer3\n",
    "        self.layer4 = model.layer4\n",
    "        self.layers = [self.layer1, self.layer2, self.layer3, self.layer4]\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        \n",
    "        pre = self.pre(x)\n",
    "        for layer in self.layers:\n",
    "            pre = layer(pre)\n",
    "            acts.append(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d57ad-9286-4908-a705-c3c4a1e84f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6de14-902a-46e9-9963-f82af8b75723",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder34 = ResNet34Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d6992-d8fa-4890-95d1-14616c4d45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71636f00-a112-4abd-9ecb-aad3985af1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yi in y:\n",
    "    print(yi.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371f798-0d3f-41ac-b9c7-1f999dc3df28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
